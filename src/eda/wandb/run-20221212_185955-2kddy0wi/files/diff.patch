diff --git a/main.py b/main.py
index 2de300a..9a91382 100644
--- a/main.py
+++ b/main.py
@@ -1,28 +1,31 @@
+# =============================================================================
+# IMPORT MODULES
+# =============================================================================
 import json
-
 import mlflow
 import tempfile
 import os
-import wandb
 import hydra
 from omegaconf import DictConfig
 
+# =============================================================================
+# PIPELINE STEPS
+# =============================================================================
 _steps = [
     "download",
     "basic_cleaning",
     "data_check",
     "data_split",
     "train_random_forest",
-    # NOTE: We do not include this in the steps so it is not run by mistake.
-    # You first need to promote a model export to "prod" before you can run this,
-    # then you need to run this step explicitly
-    #    "test_regression_model"
+    "test_regression_model"
 ]
-
 # aliases
 join = os.path.join
 
 
+# =============================================================================
+# MAIN: HYDRA LINKS THE INDIVIDUAL MLFLOW COMPONENTS
+# =============================================================================
 # This automatically reads in the configuration
 @hydra.main(config_name='config')
 def go(config: DictConfig):
@@ -102,16 +105,12 @@ def go(config: DictConfig):
 
         if "train_random_forest" in active_steps:
 
-            # NOTE: we need to serialize the random forest configuration into JSON
             rf_config = os.path.abspath("rf_config.json")
             with open(rf_config, "w+") as fp:
                 # DO NOT TOUCH
                 json.dump(
                     dict(config["modeling"]["random_forest"].items()), fp)
 
-            # NOTE: use the rf_config we just created as the rf_config parameter for the train_random_forest
-            # step
-
             _ = mlflow.run(
                 os.path.join(root_dir, "src", "train_random_forest"),
                 "main",
@@ -137,5 +136,8 @@ def go(config: DictConfig):
             )
 
 
+# =============================================================================
+# CALL THE MAIN
+# =============================================================================
 if __name__ == "__main__":
     go()
diff --git a/src/basic_cleaning/run.py b/src/basic_cleaning/run.py
index 8a98c65..5cef18a 100644
--- a/src/basic_cleaning/run.py
+++ b/src/basic_cleaning/run.py
@@ -2,6 +2,9 @@
 """
 Download from W&B the raw dataset and apply some basic data cleaning, exporting the result to a new artifact
 """
+# =============================================================================
+# IMPORT MODULES
+# =============================================================================
 import argparse
 import logging
 import tempfile
@@ -60,6 +63,9 @@ def go(args):
     artifact.wait()
 
 
+# =============================================================================
+# CALL MAIN
+# =============================================================================
 if __name__ == "__main__":
 
     parser = argparse.ArgumentParser(description="A very basic data cleaning")
diff --git a/src/data_check/test_data.py b/src/data_check/test_data.py
index 1f4a37a..cc2c2c9 100644
--- a/src/data_check/test_data.py
+++ b/src/data_check/test_data.py
@@ -1,3 +1,6 @@
+# =============================================================================
+# IMPORT MODULES
+# =============================================================================
 import pandas as pd
 import numpy as np
 import scipy.stats
diff --git a/src/train_random_forest/feature_engineering.py b/src/train_random_forest/feature_engineering.py
deleted file mode 100644
index d4a1c73..0000000
--- a/src/train_random_forest/feature_engineering.py
+++ /dev/null
@@ -1,11 +0,0 @@
-import pandas as pd
-import numpy as np
-
-
-def delta_date_feature(dates):
-    """
-    Given a 2d array containing dates (in any format recognized by pd.to_datetime), it returns the delta in days
-    between each date and the most recent date in its column
-    """
-    date_sanitized = pd.DataFrame(dates).apply(pd.to_datetime)
-    return date_sanitized.apply(lambda d: (d.max() -d).dt.days, axis=0).to_numpy()
diff --git a/src/train_random_forest/run.py b/src/train_random_forest/run.py
index c3999ad..1fdf0bf 100644
--- a/src/train_random_forest/run.py
+++ b/src/train_random_forest/run.py
@@ -2,6 +2,9 @@
 """
 This script trains a Random Forest
 """
+# =============================================================================
+# MODULES
+# =============================================================================
 import argparse
 import logging
 import os
@@ -26,18 +29,25 @@ from sklearn.metrics import mean_absolute_error
 from sklearn.pipeline import Pipeline, make_pipeline
 
 
+# =============================================================================
+# FEATURE ENGINEERING
+# =============================================================================
 def delta_date_feature(dates):
     """
     Given a 2d array containing dates (in any format recognized by pd.to_datetime), it returns the delta in days
     between each date and the most recent date in its column
     """
     date_sanitized = pd.DataFrame(dates).apply(pd.to_datetime)
-    return date_sanitized.apply(lambda d: (d.max() -d).dt.days, axis=0).to_numpy()
+    return date_sanitized.apply(lambda d: (d.max() - d).dt.days, axis=0).to_numpy()
 
 
 logging.basicConfig(level=logging.INFO, format="%(asctime)-15s %(message)s")
 logger = logging.getLogger()
 
+# =============================================================================
+# MAIN
+# =============================================================================
+
 
 def go(args):
 
@@ -52,11 +62,12 @@ def go(args):
     # Fix the random seed for the Random Forest, so we get reproducible results
     rf_config['random_state'] = args.random_seed
 
-     # Get artifact and save file in train_local_path
+    # Get artifact and save file in train_local_path
     trainval_local_path = run.use_artifact(args.trainval_artifact).file()
 
     X = pd.read_csv(trainval_local_path)
-    y = X.pop("price")  # this removes the column "price" from X and puts it into y
+    # this removes the column "price" from X and puts it into y
+    y = X.pop("price")
 
     logger.info(f"Minimum price: {y.min()}, Maximum price: {y.max()}")
 
@@ -66,12 +77,13 @@ def go(args):
 
     logger.info("Preparing sklearn pipeline")
 
-    sk_pipe, processed_features = get_inference_pipeline(rf_config, args.max_tfidf_features)
+    sk_pipe, processed_features = get_inference_pipeline(
+        rf_config, args.max_tfidf_features)
 
     # Then fit it to the X_train, y_train data
     logger.info("Fitting")
 
-    # Fit the pipeline sk_pipe 
+    # Fit the pipeline sk_pipe
     sk_pipe.fit(X_train, y_train)
 
     # Compute r2 and MAE
@@ -92,12 +104,12 @@ def go(args):
 
     # Infer the signature of the model
     signature = infer_signature(X_val, y_pred)
-    
+
     # Save the sk_pipe pipeline
     mlflow.sklearn.save_model(sk_pipe,
-                                "random_forest_dir", 
-                                signature=signature,
-                                input_example=X_val.iloc[:2])
+                              "random_forest_dir",
+                              signature=signature,
+                              input_example=X_val.iloc[:2])
 
     # Create and log W&B artifact
     artifact = wandb.Artifact(
@@ -121,21 +133,27 @@ def go(args):
     # Upload to W&B the feture importance visualization
     run.log(
         {
-          "feature_importance": wandb.Image(fig_feat_imp),
+            "feature_importance": wandb.Image(fig_feat_imp),
         }
     )
 
+# =============================================================================
+# FEATURE IMPORTANCE
+# =============================================================================
+
 
 def plot_feature_importance(pipe, feat_names):
     # We collect the feature importance for all non-nlp features first
     feat_imp = pipe["random_forest"].feature_importances_[: len(feat_names)-1]
     # For the NLP feature we sum across all the TF-IDF dimensions into a global
     # NLP importance
-    nlp_importance = sum(pipe["random_forest"].feature_importances_[len(feat_names) - 1:])
+    nlp_importance = sum(pipe["random_forest"].feature_importances_[
+                         len(feat_names) - 1:])
     feat_imp = np.append(feat_imp, nlp_importance)
     fig_feat_imp, sub_feat_imp = plt.subplots(figsize=(10, 10))
     # idx = np.argsort(feat_imp)[::-1]
-    sub_feat_imp.bar(range(feat_imp.shape[0]), feat_imp, color="r", align="center")
+    sub_feat_imp.bar(
+        range(feat_imp.shape[0]), feat_imp, color="r", align="center")
     _ = sub_feat_imp.set_xticks(range(feat_imp.shape[0]))
     _ = sub_feat_imp.set_xticklabels(np.array(feat_names), rotation=90)
     fig_feat_imp.tight_layout()
@@ -161,7 +179,7 @@ def get_inference_pipeline(rf_config, max_tfidf_features):
         SimpleImputer(strategy="most_frequent"),
         OneHotEncoder()
     )
-    
+
     # Let's impute the numerical columns to make sure we can handle missing values
     # (note that we do not scale because the RF algorithm does not need that)
     zero_imputed = [
@@ -181,7 +199,8 @@ def get_inference_pipeline(rf_config, max_tfidf_features):
     # a review for a long time), and then we create a new feature from it,
     date_imputer = make_pipeline(
         SimpleImputer(strategy='constant', fill_value='2010-01-01'),
-        FunctionTransformer(delta_date_feature, check_inverse=False, validate=False)
+        FunctionTransformer(delta_date_feature,
+                            check_inverse=False, validate=False)
     )
 
     # Some minimal NLP for the "name" column
@@ -200,7 +219,8 @@ def get_inference_pipeline(rf_config, max_tfidf_features):
     preprocessor = ColumnTransformer(
         transformers=[
             ("ordinal_cat", ordinal_categorical_preproc, ordinal_categorical),
-            ("non_ordinal_cat", non_ordinal_categorical_preproc, non_ordinal_categorical),
+            ("non_ordinal_cat", non_ordinal_categorical_preproc,
+             non_ordinal_categorical),
             ("impute_zero", zero_imputer, zero_imputed),
             ("transform_date", date_imputer, ["last_review"]),
             ("transform_name", name_tfidf, ["name"])
@@ -208,7 +228,8 @@ def get_inference_pipeline(rf_config, max_tfidf_features):
         remainder="drop",  # This drops the columns that we do not transform
     )
 
-    processed_features = ordinal_categorical + non_ordinal_categorical + zero_imputed + ["last_review", "name"]
+    processed_features = ordinal_categorical + \
+        non_ordinal_categorical + zero_imputed + ["last_review", "name"]
 
     # Create random forest
     random_Forest = RandomForestRegressor(**rf_config)
@@ -224,6 +245,9 @@ def get_inference_pipeline(rf_config, max_tfidf_features):
     return sk_pipe, processed_features
 
 
+# =============================================================================
+# CALL MAIN
+# =============================================================================
 if __name__ == "__main__":
 
     parser = argparse.ArgumentParser(description="Basic cleaning of dataset")
